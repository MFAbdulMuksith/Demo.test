

On an Ubuntu server, logs are usually stored in the `/var/log` directory, which contains logs for the system, services, and various applications. Here’s a breakdown of common logs you’ll find there and what they track:

### 1. **System Logs**
   - **`/var/log/syslog`**: General system activity logs, including system messages and events. This file is often the primary log file for most distributions.
   - **`/var/log/messages`**: Similar to `syslog`, but not always present in Ubuntu. Tracks general system events.
   - **`/var/log/kern.log`**: Kernel-related logs. Useful for tracking kernel-related issues and hardware events.
   - **`/var/log/dmesg`**: Logs from the boot sequence, especially related to hardware and kernel initialization. This file contains diagnostic messages related to hardware and can be viewed using the `dmesg` command.

### 2. **Authentication Logs**
   - **`/var/log/auth.log`**: Contains authentication-related events, including successful and failed login attempts, sudo usage, and SSH activity.

### 3. **Boot Logs**
   - **`/var/log/boot.log`**: Logs from the system startup process. This file is helpful for diagnosing startup or boot issues.

### 4. **Daemon Logs**
   - **`/var/log/daemon.log`**: Logs from system daemons, which are background processes. This includes logs from services like cron, dhclient, and others.

### 5. **Application and Service Logs**
   - **`/var/log/apache2/`** or **`/var/log/nginx/`**: Logs for web servers such as Apache and Nginx.
   - **`/var/log/mysql/`** or **`/var/log/mariadb/`**: Logs for MySQL or MariaDB databases.
   - **`/var/log/postgresql/`**: Logs for PostgreSQL databases.
   - **`/var/log/cron.log`**: Logs for scheduled cron jobs.
   - **`/var/log/mail.log`**: Logs for mail server activity if the server is configured as a mail server.
  
### 6. **Package Manager Logs**
   - **`/var/log/apt/`**: Logs from the APT package manager. Files like `history.log` and `term.log` show recent package installations, upgrades, and removals.

### 7. **Audit Logs**
   - **`/var/log/audit/audit.log`**: Logs generated by the `auditd` service if enabled. Provides security and access-related logs and is often used in compliance tracking.

### 8. **Custom Logs**
   - If custom applications or scripts are running on the server, they may store logs in `/var/log` or in other specific directories, depending on the application's configuration.

### Viewing Logs in Real-Time
To view logs as they are updated, use:
```bash
tail -f /var/log/syslog
```

### Other Useful Commands
- **`journalctl`**: For managing and viewing `systemd` logs (in systems that use `systemd` for logging).
   - Example: `journalctl -xe` to view recent system events

---

To add log monitoring capabilities to your existing Prometheus and Grafana stack, you can integrate **Grafana Loki** as a log aggregation tool. Loki works seamlessly with Prometheus for metrics and with Grafana for visualizing both metrics and logs in a unified interface. Here’s how you can modify your Docker Compose setup to include Loki and Grafana Loki's log aggregation.

---

### 1. Update Your Docker Compose File to Include Loki and Promtail

Loki will act as the log aggregation tool, while **Promtail** will be used as an agent to ship logs to Loki. You’ll need to add Loki and Promtail services to your `docker-compose.yml` file.

#### Updated Docker Compose Configuration

Here’s the modified `docker-compose.yml` file that includes Loki and Promtail:

```yaml
version: '3.9'

services:
  prometheus:
    image: prom/prometheus:v2.40.2
    restart: always
    container_name: prometheus
    ports:
      - 9090:9090
    volumes:
      - /opt/container/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - /opt/container/prometheus/alert.rules.yml:/etc/prometheus/alert.rules.yml
      - /opt/container/prometheus/data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=90d'
      - '--web.enable-lifecycle'
    networks:
      - monitor

  alert:
    image: prom/alertmanager:v0.24.0
    restart: always
    container_name: alertmanager
    ports:
      - 9093:9093
    volumes:
      - /opt/container/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
    networks:
      - monitor

  grafana:
    image: grafana/grafana:9.2.5
    restart: always
    container_name: grafana
    ports:
      - 3000:3000
    volumes:
      - /opt/container/grafana/data:/var/lib/grafana
    environment:
      - GF_INSTALL_PLUGINS=grafana-loki  # Install the Loki plugin for Grafana
    labels:
      - traefik.enable=true
      - traefik.http.routers.grafana.rule=Host(`monitor.ascension-holding.com`)
      - traefik.http.routers.grafana.entrypoints=web
      - traefik.http.routers.grafana.entrypoints=websecure
      - traefik.http.routers.grafana.tls=true
      - traefik.http.routers.grafana.tls.certresolver=myresolver
    networks:
      - monitor

  loki:
    image: grafana/loki:2.4.1
    container_name: loki
    restart: always
    ports:
      - 3100:3100
    volumes:
      - /opt/container/loki/config.yml:/etc/loki/config.yml
    command: -config.file=/etc/loki/config.yml
    networks:
      - monitor

  promtail:
    image: grafana/promtail:2.4.1
    container_name: promtail
    restart: always
    volumes:
      - /var/log:/var/log  # Adjust path as needed
      - /opt/container/promtail/config.yml:/etc/promtail/config.yml
    command: -config.file=/etc/promtail/config.yml
    networks:
      - monitor

  # Other services...

networks:
  monitor:
    external: true
```

---


### 2. Create Loki and Promtail Configuration Files

1. **Loki Configuration** (`/opt/container/loki/config.yml`):
   ```yaml
   auth_enabled: false
   server:
     http_listen_port: 3100
   ingester:
     lifecycler:
       ring:
         kvstore:
           store: inmemory
         replication_factor: 1
     chunk_idle_period: 5m
     max_chunk_age: 1h
     chunk_target_size: 1048576
     chunk_retain_period: 30s
   schema_config:
     configs:
       - from: 2020-10-24
         store: boltdb-shipper
         object_store: filesystem
         schema: v11
         index:
           prefix: index_
           period: 168h
   storage_config:
     boltdb_shipper:
       active_index_directory: /tmp/loki/index
       cache_location: /tmp/loki/boltdb-cache
       cache_ttl: 24h
     filesystem:
       directory: /tmp/loki/chunks
   limits_config:
     enforce_metric_name: false
     max_cache_freshness_per_query: 10m
   ```
   - Adjust paths and configurations as necessary for your environment.

2. **Promtail Configuration** (`/opt/container/promtail/config.yml`):
   ```yaml
   server:
     http_listen_port: 9080
     grpc_listen_port: 0

   positions:
     filename: /tmp/positions.yaml

   clients:
     - url: http://loki:3100/loki/api/v1/push

   scrape_configs:
     - job_name: system
       static_configs:
         - targets:
             - localhost
           labels:
             job: varlogs
             __path__: /var/log/*.log  # Adjust path to log files as needed
   ```

---

### 3. Update Grafana to Use Loki as a Data Source

1. Access Grafana by navigating to `http://localhost:3000`.
2. Login with default credentials (`admin`/`admin`).
3. Go to **Configuration > Data Sources**.
4. Add a new **Loki** data source:
   - URL: `http://loki:3100`
5. Save and test the Loki data source.

### 4. Create Dashboards for Logs and Metrics

- **Log Panels**: In Grafana, create panels with queries to retrieve logs from Loki. Use **LogQL** to filter and search logs.
  - Example Query: `{job="varlogs"}` to display all logs collected by Promtail.

- **Metrics Panels**: Use Prometheus as the data source and build panels for metrics as you have already set up.

---

### 5. Validate the Setup

- Check if logs are flowing from Promtail to Loki.
- Ensure that Grafana is visualizing both logs (from Loki) and metrics (from Prometheus).
- Test log searches and explore LogQL queries for more refined log insights.

---

### Summary of Key Changes

- Added **Loki** as a log aggregator and **Promtail** as a log shipper.
- Configured Grafana to use both Prometheus (for metrics) and Loki (for logs) as data sources.
- Updated Docker Compose to manage the monitoring stack as a cohesive unit.

This setup allows you to have centralized log monitoring integrated with Prometheus and Grafana, enabling easy correlation of logs with metrics for more effective troubleshooting and observability.

---

To integrate logging with this stack, you can expand it by adding tools like **Loki** for log aggregation, **Promtail** for log scraping, and connecting **Grafana** to visualize logs. Here's a step-by-step guide to set this up:

### Step 1: Add Loki and Promtail to `docker-compose.yaml`

1. **Loki** will be responsible for storing and indexing logs.
2. **Promtail** is an agent that will collect log data from various sources and send it to Loki.

Update your `docker-compose.yaml` as follows:

```yaml
version: '3.9'

services:
  # Existing services remain the same

  # Add Loki service
  loki:
    image: grafana/loki:2.6.1
    restart: always
    container_name: loki
    ports:
      - 3100:3100
    volumes:
      - /opt/container/loki/config.yml:/etc/loki/local-config.yaml # Loki config
      - /opt/container/loki/data:/loki # Loki data store
    command:
      - '--config.file=/etc/loki/local-config.yaml'
    networks:
      - monitor

  # Add Promtail service
  promtail:
    image: grafana/promtail:2.6.1
    restart: always
    container_name: promtail
    volumes:
      - /var/log:/var/log # Path to system logs
      - /opt/container/promtail/config.yml:/etc/promtail/config.yml # Promtail config
    command:
      - '-config.file=/etc/promtail/config.yml'
    networks:
      - monitor

networks:
  monitor:
    external: true
```

### Step 2: Configure Loki

Create a Loki configuration file (e.g., `/opt/container/loki/config.yml`):

```yaml
auth_enabled: false

server:
  http_listen_port: 3100

ingester:
  lifecycler:
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1

schema_config:
  configs:
    - from: 2022-01-01
      store: boltdb-shipper
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 24h

storage_config:
  boltdb_shipper:
    active_index_directory: /loki/index
    cache_location: /loki/cache
    shared_store: filesystem
  filesystem:
    directory: /loki/chunks

limits_config:
  enforce_metric_name: false
  max_cache_freshness_per_query: 10m

chunk_store_config:
  max_look_back_period: 0s

table_manager:
  retention_deletes_enabled: true
  retention_period: 24h
```

### Step 3: Configure Promtail

Create a Promtail configuration file (e.g., `/opt/container/promtail/config.yml`):

```yaml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /var/log/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  - job_name: system
    static_configs:
      - targets:
          - localhost
        labels:
          job: varlogs
          __path__: /var/log/*.log
```

This config will:
- Collect logs from `/var/log/*.log`
- Push them to Loki on `http://loki:3100/loki/api/v1/push`

### Step 4: Update Prometheus Config (prometheus.yml)

To integrate log monitoring with Prometheus, you can set alerts based on Loki’s log metrics by configuring Prometheus. Add this to your `prometheus.yml` configuration file:

```yaml
global:
  scrape_interval: 15s

scrape_configs:
  # Other scrape configs remain unchanged

  - job_name: 'loki'
    metrics_path: /metrics
    static_configs:
      - targets: ['loki:3100']
```

### Step 5: Update Grafana to Use Loki as a Data Source

1. Access Grafana at `http://localhost:3000` (or your Grafana URL).
2. Go to **Configuration > Data Sources**.
3. Add a new data source, select **Loki**.
4. Set the **URL** to `http://loki:3100`.
5. Save and test the data source.

Now, you can use Grafana to create visualizations for both metrics (from Prometheus) and logs (from Loki). 

### Step 6: Create Grafana Dashboards

- **Log Dashboard**: In Grafana, create a dashboard for logs. You can visualize logs from different sources and use labels to filter logs.
- **Metrics and Logs in the Same Dashboard**: You can correlate metrics from Prometheus with logs from Loki in a single dashboard.

With this setup, you’ll have a comprehensive monitoring stack that includes both metric monitoring (via Prometheus) and log monitoring (via Loki).

---

Continuing from where we left off, let’s ensure your monitoring stack is fully functional with **dashboards, alerts, and log visualization** in Grafana. Here are additional steps for refining the setup and ensuring everything is ready for use:

### Step 7: Create Dashboards and Panels in Grafana

Once Loki is set as a data source in Grafana, you can start building panels that visualize logs. Here’s how:

1. **Create a New Dashboard in Grafana**:
   - Go to **Dashboards > New Dashboard**.
   - Add a new **Panel** to the dashboard.

2. **Configure Log Panels with Loki**:
   - In the panel configuration, set **Data Source** to **Loki**.
   - Use the **Log Labels** to filter logs (such as `job="varlogs"` for Promtail logs).
   - You can filter by keywords or use regex to refine your logs and highlight important entries, such as errors or warnings.

3. **Combine Logs and Metrics in a Single Dashboard**:
   - You can combine panels with data from **Prometheus** (for metrics) and **Loki** (for logs).
   - This is particularly helpful for correlating metrics like CPU spikes or memory usage with logs, giving you insights into root causes during troubleshooting.

4. **Customize Visualization**:
   - Customize the time range, and alert thresholds, and use annotations to mark events of interest directly on your time series graphs.

### Step 8: Set Up Alerts in Grafana for Logs and Metrics

Grafana’s alerting can notify you when specific metrics or log patterns are detected. Here’s how to set it up:

1. **Enable Grafana Alerting**:
   - In **Grafana**, go to **Alerting > Alert Rules**.
   - Create a new **Alert Rule**.

2. **Configure Alerts for Logs**:
   - Choose **Loki** as the data source.
   - Set the query to match patterns in your logs. For example, look for error strings (`level="error"`) in log entries.
   - Define conditions (e.g., if the log frequency exceeds a threshold in a given time window).

3. **Configure Alerts for Metrics**:
   - Choose **Prometheus** as the data source.
   - Set conditions based on metrics (e.g., if CPU usage is above 90% for a certain duration).

4. **Set Notification Channels**:
   - In **Alerting > Notification channels**, configure where alerts should be sent, such as email, Slack, or PagerDuty.
   - Set the channel in the alert rule to receive notifications based on defined triggers.

### Step 9: Verify and Test the Setup

1. **Verify Loki and Promtail**:
   - Check the logs for both Loki and Promtail containers to confirm they’re running without issues.
   - Test log ingestion by creating sample logs (e.g., using `logger` on the host system) and verify that they appear in Grafana’s log dashboard.

2. **Verify Metrics Collection with Prometheus**:
   - In Prometheus, go to `http://localhost:9090` (or your Prometheus URL).
   - Confirm that Prometheus is scraping metrics from all configured targets, including `loki:3100` and `cadvisor`.

3. **Test Grafana Dashboards and Alerts**:
   - Simulate events (e.g., CPU load or an error log) and monitor how they appear in Grafana.
   - Verify alert notifications to ensure you’re receiving them as expected in your configured channels.

### Step 10: Set Up Dashboard Annotations for Correlation

Annotations can mark significant events or changes on Grafana dashboards, which can help with root-cause analysis.

1. **Add Annotations in Grafana**:
   - In **Dashboard Settings > Annotations**, configure annotations to mark events like **deployments, scaling actions**, or **service restarts**.
   - This lets you correlate changes in system state or configuration with spikes or changes in logs and metrics.

2. **Add Automatic Annotations via API**:
   - You can use the Grafana API to create annotations programmatically during deployments or critical events.

### Optional Enhancements

Consider adding the following tools for a more robust monitoring stack:

- **Grafana Tempo**: To capture distributed traces alongside logs and metrics, particularly useful for microservices.
- **Prometheus Alertmanager Integration**: Connect Alertmanager with Grafana for more complex alert routing and silencing.

By following these steps, your stack should provide comprehensive monitoring with metrics, logs, and alerting, making it easier to manage and troubleshoot your environment in Grafana.


---


Your `prometheus.yml` configuration looks mostly correct, with a few adjustments to consider for readability and functionality:

### Suggested Improvements and Explanations

1. **Global Configuration Comments**: 
   - Your comments mention both 15 seconds and 1 minute as the `scrape_interval`. Since it's set to `1m`, consider updating comments for consistency.

2. **Alertmanager Configuration**:
   - Make sure that the IP address `10.0.3.110` on port `9093` corresponds to your `alertmanager` service address. Otherwise, this section is correctly configured.

3. **Scrape Interval**:
   - Since some configurations need more frequent scraping (e.g., cAdvisor or node exporters for near-real-time data), you may consider overriding the `scrape_interval` for specific jobs by adding `scrape_interval` and `scrape_timeout` within each job.

4. **Blackbox Exporter Configuration**:
   - You’re using Blackbox for HTTP probing, which is correct. Make sure that the Blackbox `config.yml` contains corresponding modules `ray_ilooops` and `achieve-erp` with appropriate configurations for HTTP checks.

5. **Loki Integration**:
   - Ensure that `loki:3100` is accessible from the Prometheus container by either defining `loki` in the same Docker network or adjusting the IP if on a different host.

Here’s an updated, cleaned-up version for readability and optimal settings:

```yaml
# Prometheus Configuration
global:
  scrape_interval: 1m
  # Global scrape interval set to 1 minute. Customize as needed per job.

# Alertmanager
alerting:
  alertmanagers:
    - static_configs:
      - targets:
        - '10.0.3.110:9093'

# Rule Files
rule_files:
  - /etc/prometheus/alert.rules.yml

# Scrape Configurations
scrape_configs:

  # Scrape Prometheus itself
  - job_name: "prometheus"
    static_configs:
      - targets: ["localhost:9090"]

  # Node Exporter for Prometheus Host
  - job_name: "prometheus-host-server"
    static_configs:
      - targets: ["10.0.3.110:9100"]

  # Grafana Server Monitoring
  - job_name: "grafana-service"
    static_configs:
      - targets: ["10.0.3.110:3000"]

  # Alertmanager Monitoring
  - job_name: "alert-manager-service"
    static_configs:
      - targets: ["10.0.3.110:9093"]

  # cAdvisor for Container Metrics
  - job_name: "cadvisor-service"
    scrape_interval: 30s  # Set a faster scrape interval for cAdvisor if needed
    static_configs:
      - targets: ["10.0.3.110:8080"]

  # Blackbox Exporter Monitoring
  - job_name: "blackbox-service"
    static_configs:
      - targets: ["10.0.3.110:9115"]

  # Node Exporter for ASH-Preprod Server
  - job_name: "ASH-Preprod"
    static_configs:
      - targets: ["172.32.20.90:9100"]

  # Node Exporter for Ilooops Production
  - job_name: "ilooops-production-01"
    static_configs:
      - targets: ["10.0.2.124:9100"]

  # Node Exporter for UAT Server
  - job_name: "UAT-Server"
    static_configs:
      - targets: ["172.32.20.164:9100"]

  # HTTP Probing for Ray Ilooops (Blackbox Exporter)
  - job_name: "ray_ilooops"
    metrics_path: /probe
    params:
      module: [ray_ilooops]  # Ensure this module exists in Blackbox configuration
    static_configs:
      - targets:
          - https://ray.ilooops.com
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: 10.0.3.110:9115  # Blackbox exporter's address

  # HTTP Probing for Achieve ERP (Blackbox Exporter)
  - job_name: "achieve-erp"
    metrics_path: /probe
    params:
      module: [achieve-erp]  # Ensure this module exists in Blackbox configuration
    static_configs:
      - targets:
          - https://achieve-erp.com/
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: 10.0.3.110:9115  # Blackbox exporter's address

  # Loki Monitoring
  - job_name: "loki"
    metrics_path: /metrics
    static_configs:
      - targets: ["loki:3100"]
```

### Additional Notes

- **Blackbox Modules**: Make sure your Blackbox Exporter configuration (`config.yml`) has defined modules `ray_ilooops` and `achieve-erp`, each specifying the necessary HTTP probing settings, including `method`, `timeout`, and `failure conditions`.
  
- **Network Configuration**: Since all these services need to be reachable, ensure they are on the same Docker network (in this case, `monitor`) and the correct IPs/hostnames are used in your Prometheus scrape configurations.

This setup should now be complete for monitoring multiple services, including health checks through the Blackbox Exporter, system metrics through the Node Exporter, and container metrics through cAdvisor, all visible in Grafana with Loki for log aggregation. 

Test each endpoint and query the logs in Grafana to confirm everything is properly integrated.


---



